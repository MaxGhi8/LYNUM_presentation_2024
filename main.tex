\documentclass{beamer}
\usepackage{Config/style/uniReg}
\usepackage{tikz}
\usepackage{latexsym,amsmath,xcolor,multicol,booktabs,calligra}
\usepackage{hyperref}
\usepackage{subcaption}
\graphicspath{{figures/}}
\usetikzlibrary{positioning, arrows.meta, fit, shapes.geometric, calc}

% Use the theme color best suited for your departement. 
% Available themes (case sensitive) : Red, Yellow, GreenBrown, DarkBlue, Black&White (default)
\themecolor{Red}

% Change visibility of the logo in the background. Value can be 0, 1 or 2
\visibilitySiegel{0}

% title and subtitle
\title{Operator learning \newline for multi-patch domains}
\subtitle{}

% For multiple authors/presenters :
%   1. Comment \singleAuthor and use \multAuthors as in example
%   2. a- For same faculty/institute, you can use \AuthorInstitute{...}
%         instead if of \inst{...}
%      b- For different institutes, comment the line \AuthorInstitute{...} and use
%         \inst{...} together with \institute{...}
\singleAuthor{Massimiliano Ghiotto}
\AuthorInstitute{PhD student at University of Pavia}
%\multAuthors{Antoine Gansel\inst{1} \and Jullie Cailler\inst{2}}

%% collaborators with institute
% \Collaborators{{Julie Cailler\inst{2}}} 
% \Supervisors{Carlo Marcati\inst{1} \and {Giancarlo Sangalli\inst{1}}}
% \institute{{\inst{1}University of Pavia}}
%% collaborators without institute
\Supervisors{Carlo Marcati, \and Giancarlo Sangalli} % \institute{{\inst{1}University of Pavia}}

\newcommand{\numberset}{\mathbb}
\newcommand{\N}{\numberset{N}}
\newcommand{\Z}{\numberset{Z}}
\newcommand{\Q}{\numberset{Q}}
\newcommand{\R}{\numberset{R}}
\newcommand{\C}{\numberset{C}}

\begin{document}

\frame{\titlepage}

\addtocounter{framenumber}{-1}
\setbeamertemplate{footline}[footline-body]
\setbeamertemplate{background}[background-body]

%% ======================================

\section{Operator Learning}

%% ======================================

% \cite{kovachki2021neural}\blfootnote{\cite{kovachki2021neural}, Kovachki et al., "Neural operator: Learning maps between function spaces".}
\begin{frame}[t]{Definition of Neural Operator}
	\vspace{-0.5cm}
	\[ \mathcal{N}_{\theta} :\mathcal{A}(D, \R^{d_a}) \to \mathcal{U}(D, \R^{d_{u}}), \quad \text{with } D \subset \R^d, \quad	\mathcal{N}_{\theta} := \mathcal{Q} \circ \mathcal{L}_L \circ \cdots \circ \mathcal{L}_1 \circ \mathcal{P} . \]
	\vspace{-0.2cm}
	\begin{enumerate}
		\item \textbf{Lifting:} linear and local operator 
			\[\mathcal{P}:\, \mathcal{A}(D, \R^{d_a}) \to \mathcal{U}(D, \R^{d_{v}}), \quad  \mathcal{P}(a)(x) = P\cdot a(x), \ P \in \R^{d_{v} \times d_a} \]
	\vspace{-0.2cm}
		\pause
		\item \textbf{Integral operator:} for $ t = 1, \dots, L $
		\[ \mathcal{L}_t : \, \mathcal{U}(D, \R^{d_{v}}) \to  \mathcal{U}(D, \R^{d_{v}}) \]
		\[  \mathcal{L}_t(v)(x) := \sigma\Big( W_t v(x)+ b_t(x) + (\mathcal{K}_t(a, \theta) v)(x) \Big) \]
		with $ \mathcal{K}_t(a, \theta) $ linear and non-local operator.  
		\pause
		\item \textbf{Projection:} linear and local operator 
		\[\mathcal{Q}:\, \mathcal{U}(D_{L}, \R^{d_{v}}) \to  \mathcal{U}(D, \R^{d_{u}}), \quad  \mathcal{Q}(v)(x) = Q\cdot v(x), \ Q \in \R^{d_{v_u} \times d_{v}}. \]		
	\end{enumerate}
\end{frame}

%% ======================================

\begin{frame}{Integral operator}
	There are different ways to define the integral operator $ \mathcal{K}_t $:
	\begin{itemize}
		\item defining $ \kappa_{t,\theta} \in C(D\times D,\, \R^{d_{v}\times d_{v}}) $
		\[ (\mathcal{K}_t(a, \theta)v)(x) = (\mathcal{K}_t(\theta)v)(x) = \int_{D} \kappa_{t,\theta}(x, y) v(y) \ d\mu_t(y).\]
		\pause
		\item defining $ \kappa_{t,\theta} \in C(D\times D\times \R^{d_a} \times \R^{d_a},\, \R^{d_{v}\times d_{v}}) $
		\[	(\mathcal{K}_t(a, \theta)v)(x) = \int_{D} \kappa_{t, \theta}\left( x, y, a\left(x\right), a\left(y\right) \right)v(y)  \ d\mu_t(y).\]
		\pause
		\item defining $ \kappa_{t,\theta} \in C(D\times D\times \R^{d_{v}} \times \R^{d_{v}},\, \R^{d_{v}\times d_{v}}) $
		\[ (\mathcal{K}_t(a, \theta)v)(x) = \int_{D} \kappa_{t,\theta}\left( x, y, v\left(x\right), v(y) \right) v(y)  \ d\mu_t(y). \]
	\end{itemize}
\end{frame}

%% ======================================

% \begin{frame}
% 	% \blfootnote{\cite{kovachki2021neural}, Kovachki et al., "Neural operator: Learning maps between function spaces".}
% 	\begin{themedTitleBlock}{Teorema}%\cite{kovachki2021neural}
% 		Sono casi particolari di operatori neurali:
% 		\begin{itemize}
% 			\item le reti neurali classiche;
% 			\item le reti neurali convoluzionali;
% 			\item i transformers;
% 			\item le DeepONets;
% 		\end{itemize}
% 	\end{themedTitleBlock}
% \end{frame}

%% ======================================
\section{Fourier Neural Operator}
%% ======================================

\begin{frame}{Definition of Fourier Neural Operator}
	For defining the Fourier Neural Operator (FNO) we make the first assumption and the further assumption that $\kappa_{t,\theta_t}(x, y) = \kappa_{t,\theta_t}(x -y)$,
	\[ (\mathcal{K}_t(\theta_t)v)(x) = \int_{\mathbb{T}^d} \kappa_{t,\theta_t}(x-y) v(y) \ dy = (\kappa_{t, \theta_t} * v)(x)  .\]
	\pause
	Using the convolution theorem we have
	\[ (\kappa_{t, \theta_t} * v)(x) =  \mathcal{F}^{-1}\left( \mathcal{F}( \kappa_{t,\theta_t}) (k) \cdot \mathcal{F}(v)(k) \right)(x), \]
	and parameterizing $\mathcal{F}( \kappa_{t, \theta_t} )$ with the parameters $R_{\theta_t}(k) \in \C^{d_v \times d_v} \ \forall k \in \Z^d $, we have 
	\[  (\mathcal{K}_t(\theta_t)v)(x)= \mathcal{F}^{-1}\left( R_{\theta_t}(k) \cdot \mathcal{F}(v)(k) \right)(x) \]
\end{frame}

%% ======================================

\begin{frame}{Universal approximation for FNOs}% \blfootnote{\cite{kovachki2021universal}, Kovachki et al., "On universal approximation and error bounds for fourier neural operators".}
\begin{themedTitleBlock}{Universal approximation theorem} % \cite{kovachki2021universal}
	Given $ s, s' \ge 0 $ and 
	\[ \mathcal{G}: H^s(\mathbb{T}^d, \R^{d_a}) \to H^{s'}(\mathbb{T}^d, \R^{d_u}) \]
	continuous operator. Given $ K \subset H^s(\mathbb{T}^d, \R^{d_a}) $ a compact subset and $ \sigma \in \C^{\infty}(\R) $ non-linear and globally Lipschitz activation function. Then, for all $ \varepsilon > 0 $, exists a Fourier Neural Operator
	\[ \mathcal{N}: H^s(\mathbb{T}^d, \R^{d_a}) \to H^{s'}(\mathbb{T}^d, \R^{d_u}) \]
	such that:
	\[ \underset{a \in K}{sup}\, \left\| \mathcal{G}(a) - \mathcal{N}(a) \right\|_{H^{s'}} \le \varepsilon.  \]
\end{themedTitleBlock}	
\end{frame}

%% ======================================

\begin{frame}{Pseudo Spectral Fourier Neural Operator}
	Pseudo Spectral Fourier Neural Operator ($\psi$-FNO) is a map
	\[ \mathcal{N}^{*}: \mathcal{A}(\mathbb{T}^d, \R^{d_a}) \to \mathcal{U}(\mathbb{T}^d, \R^{d_u}), \qquad a \mapsto \mathcal{N}^{*}(a), \]
	defined by	
	\[\mathcal{N}^{*}(a) = \mathcal{Q} \circ I_N \circ \mathcal{L}_L \circ I_N \circ \dots \circ \mathcal{L}_1 \circ I_N \circ \mathcal{R}(a),	\]
	where $ I_N $ denotes the pseudo-spectral projection on the Fourier polynomials of degree $N$
	\[ I_N : C(\mathbb{T}^d) \to L^{2}_{N}(\mathbb{T}^{d}), \quad u \mapsto I_Nu. \]
	\pause
	So a $ \psi $-FNO can be identified with a finite-dimensional map 
	\[ \widetilde{\mathcal{N}}^{*}: \R^{d_a \times \mathcal{I}_N} \to \R^{d_u \times \mathcal{I}_N}, \quad \widetilde{\mathcal{N}}^{*}(a)_j = \mathcal{N}^{*}(a)(x_j) \quad \forall j \in \mathcal{I}_N, \]
	where the input $a = \{a_j\}_{j\in \mathcal{I}_N}$, $a_j = a(x_j)$ and $ \mathcal{I}_N = \left\lbrace 1, \dots, 2N \right\rbrace^d $.
\end{frame}

%% ======================================

\begin{frame}{Universal approximation for $\psi$-FNOs}
	% \blfootnote{\cite{kovachki2021universal}, Kovachki et al., "On universal approximation and error bounds for fourier neural operators".}
	\begin{themedTitleBlock}{Universal approximation for $\psi$-FNOs} %\cite{kovachki2021universal}
		Given $ s > d/2$ , $ s' \ge 0 $ and 
		\[ \mathcal{G}: H^s(\mathbb{T}^d, \R^{d_a}) \to H^{s'}(\mathbb{T}^d, \R^{d_u}) \]
		continuous operator. Given $ K \subset H^s(\mathbb{T}^d, \R^{d_a}) $ a compact set and $ \sigma \in \C^{\infty}(\R) $ a non-linear and globally Lipschitz continuous function. Then, for any $ \varepsilon > 0 $, exists an $ N \in \N $ such that the $ \psi $-FNO 
		\[ \mathcal{N}^{*}: L^2_N(\mathbb{T}^d, \R^{d_a}) \to L^2_N(\mathbb{T}^d, \R^{d_u}) \]
		satisfy:
		\[ \underset{a \in K}{sup}\, \left\| \mathcal{G}(a) - \mathcal{N}^{*}(a) \right\|_{H^{s'}} \le \varepsilon.  \]
	\end{themedTitleBlock}	
\end{frame}

%% ======================================

\begin{frame}{Representation of $\psi$-FNO}
    \begin{center}
    \begin{tikzpicture}
        \tikzset{
            box/.style={draw, rounded corners, align=center, minimum height=0.8cm, minimum width=1cm, fill=orange!30},
            bigbox/.style={draw, rounded corners, align=center, minimum height=2cm, minimum width=1cm, fill=yellow!30},
            node_sum/.style={draw, circle, fill=white, inner sep=0pt, minimum size=4mm},
            every node/.style={font=\small}
        }
        % Nodes for the main structure
        \node[box, label=above:{\textit{Input}}] (input) {$f(x)$};
        \node[box, right=0.5cm of input, label=above:{\textit{Lifting}}] (lifting) { $\mathcal{P}$ };
        \node[bigbox, right=0.5cm of lifting] (fourier1) {$\mathcal{L}_{1}$};
        \node[bigbox, right=0.55cm of fourier1] (fourier2) {$\mathcal{L}_{t}$};
        \node[bigbox, right=0.55cm of fourier2] (fourier3) {$\mathcal{L}_{L}$};
        \node[box, right=0.5cm of fourier3, label=above:{\textit{Projection}}] (projection) {$\mathcal{Q}$};
        \node[box, right=0.5cm of projection, label=above:{\textit{Output}}] (output) {$u(x)$};
        
        % Draw arrows between nodes
        \draw[->, line width = .7pt] ($(input.east)+(0.05, 0)$) -- ($(lifting.west)-(0.05,0)$);
        \draw[->, line width = .7pt] ($(lifting.east)+(0.05, 0)$) -- ($(fourier1.west)-(0.03,0)$);
        \draw[dotted, line width = 2pt] ($(fourier1.east)+(0.1, 0)$) -- ($(fourier2.west)-(0.1,0)$);
        \draw[dotted, line width = 2pt] ($(fourier2.east)+(0.1, 0)$) -- ($(fourier3.west)-(0.1,0)$);
        \draw[->, line width = .7pt] ($(fourier3.east)+(0.05, 0)$) -- ($(projection.west)-(0.05,0)$);
        \draw[->, line width = .7pt] ($(projection.east)+(0.05, 0)$) -- ($(output.west)-(0.05,0)$);
        
        % Annotations for the Fourier Layers
        \node[align=center, above=0.2cm of fourier2, font=\footnotesize] { \textit{Fourier Layers} };
        
        % Internal structure bounding box
        \node[draw, line width = .7pt, rounded corners, inner sep=0.2cm, fit= (fourier1) (fourier2) (fourier3)] (internal) {};

        %%% Second part of the plot
        % Internal structure bounding box
		\node[draw, below=0.2cm of internal, rounded corners, inner sep=0.2cm, fill=yellow!30] (internal) {
			\begin{tikzpicture}[every node/.style={font=\small}]
				% Nodes for the internal structure
				\node[box] (vt) {$v(x)$};
				\node[box, right=1cm of vt] (transform) {$\mathcal{F}$};
				\node[box, right=0.5cm of transform, fill=green!25] (nonlinear) { $ R_{\theta_t} $ };
				\node[box, right=0.5cm of nonlinear] (invtransform) { $ \mathcal{F}^{-1} $ };
				\node[box, below=0.8cm of nonlinear, fill=green!25] (linear) { $ W_t, b_t $ };

                % Internal structure bounding box
                \node[draw, line width = .7pt, rounded corners, inner sep=0.15cm, fit= (transform) (nonlinear) (invtransform) ] (diagonalscaling) {};

				\node[node_sum, right=0.5cm of invtransform] (node_sum) {$\mathbf{+}$};
				\node[box, right=0.5cm of node_sum] (activation) {$\sigma$};
				\node[box, right=0.5cm of activation] (vtplusone) {$v(x)$};
				
				% Draw arrows between nodes
				\draw[line width = .7pt] ($(vt.east)+(0.05, 0)$) -- (diagonalscaling);
				\draw[->, line width = .7pt] ($(transform.east)+(0.05, 0)$) -- ($(nonlinear.west)-(0.05, 0)$);
				\draw[->, line width = .7pt] ($(nonlinear.east)+(0.05, 0)$) -- ($(invtransform.west)-(0.05, 0)$);
				\draw[->, line width = .7pt] ($(vt.south)-(0, 0.05)$) |- ($(linear.west)-(0.05, 0)$);
				\draw[->, line width = .7pt] (diagonalscaling) -- ($(node_sum.west)-(0.05, 0)$);
				\draw[->, line width = .7pt] ($(linear.east)+(0.05, 0)$) -| ($(node_sum.south)-(0, 0.05)$);
				\draw[->, line width = .7pt] ($(node_sum.east)+(0.05, 0)$) -- ($(activation.west)-(0.05, 0)$);
				\draw[->, line width = .7pt] ($(activation.east)+(0.05, 0)$) -- ($(vtplusone.west)-(0.05, 0)$);

			\end{tikzpicture}
		};
        % Connection between the two parts
        \draw[] ($(internal.north west)+(0.1, 0.05)$) -- (fourier2.south west);
        \draw[] ($(internal.north east)+(-0.1, 0.05)$) -- (fourier2.south east);
    \end{tikzpicture}
    \end{center}
\end{frame}

%% ======================================

\section{Multi-patch domains}

%% ======================================

\begin{frame}{Fourier continuation}
	FNOs are limited to rectangular domains. To extend the operator to irregular domains, we can extend the domain to a larger rectangular domain and training the FNO to approximate the solution only on the original domain.
	\begin{themedTitleBlock}{Periodic extension operator}
		Let $\Omega \subset \R^d$ be a bounded and Lipschitz domain. There exists a continuous. linear operator $\mathcal{E}: W^{m,p}(\Omega) \to W^{m,p}(B)$ for any $m \ge 0$ and $1 \le p < \infty$, where $B$ is an hyper-cube containing $\Omega$, such that, for any $u \in W^{m,p}(\Omega)$:
		\begin{itemize}
			\item $\mathcal{E}(u)_{|_{\Omega}} = u$, 
			\item $\mathcal{E}(u)$ is periodic on $B$, including its derivatives.
		\end{itemize}
	\end{themedTitleBlock}
	\pause	
	But this can be computationally expensive.

\end{frame}


%% ======================================

\begin{frame}{Chebyshev Neural Operator}
	Let $ D \subset \R^d $ be a bounded domain. The Chebyshev Neural Operator (CNO) is a map
	\[ \mathcal{N}_{\theta}: \mathcal{A}(D, \R^{d_a}) \to \mathcal{U}(D, \R^{d_u}), \]
	defined by
	\[ \mathcal{N}_{\theta}(a) = \mathcal{Q} \circ \mathcal{L}_L \circ \cdots \circ \mathcal{L}_1 \circ \mathcal{P}(a), \]
	where is all defined as the FNO but in the integral operator we use the Chebyshev transfor and anti-transform instead of the Fourier transform, i.e.
	\[  (\mathcal{K}_t(\theta_t)v)(x)= \mathcal{C}^{-1}\left( R_{\theta_t}(k) \cdot \mathcal{C}(v)(k) \right)(x), \quad R_{\theta_t}(k) \in \R^{d_v \times d_v} \ \forall k \in \N^d. \]
	\pause 
	At the discrete level, we can define the Pseudo Spectral Chebyshev Neural Operator ($\psi$-CNO) as before, with the difference that the grid $ \mathcal{I}_N $ is defined on a Chebyshev grid and not on an uniform one.
\end{frame}

%% ======================================

\begin{frame}{Multi-patch domains}
	If we have the domain $D$ divided in $N$ patches $D_k$ with $D = \bigcup_{k=1}^{N} D_k$ with $D_i \cap D_j = \emptyset$ or $D_i \cap D_j = \Gamma_{ij}$ for $i \neq j$.
	\begin{columns}
		\begin{column}{0.5\textwidth}
			\begin{center}
				\begin{tikzpicture}

					% Define the size of the squares
					\def\squareSize{2}
					
					% Draw the L-shaped domain
					\draw[thick] (0,0) rectangle (\squareSize,\squareSize); % Bottom left square
					\draw[thick] (0,\squareSize) rectangle (\squareSize,2*\squareSize); % Top left square
					\draw[thick] (\squareSize, \squareSize) rectangle (2*\squareSize, 2*\squareSize); % Top right square
					
					% Label the patches
					\node at (\squareSize/2,\squareSize/2) {1};
					\node at (\squareSize/2,3*\squareSize/2) {2};
					\node at (3*\squareSize/2,3*\squareSize/2) {3};
					
				\end{tikzpicture}
			\end{center}
		\end{column}%
		\begin{column}{0.5\textwidth}
			We consider the L-domain 
			\[ D = [-1, 1]^2\setminus ([0,1]\times [-1,0]) \] 
			divided in $N=3$ patches $ D_1, D_2, D_3 $.
		\end{column}
	\end{columns}
\end{frame}

%% ======================================

\begin{frame}{Multi-patch Neural Operator}

\end{frame}

%% ======================================

\section{Numerical experiments}

%% ======================================

\begin{frame}{Problema di Darcy}
	For our numerical experiments we consider the Darcy problem:
	\[ 	\begin{cases}
		- \nabla(a \cdot \nabla u) = f,\quad &  \mathrm{in}\ D\\
		u = 0, & \mathrm{on} \ \partial D
	\end{cases} \]
	with the L-domain $ D = [-1, 1]^2\setminus ((0,1)\times (-1,0)) $, diffusion coefficient $\ a \in \mathcal{A} = L^{\infty}(D, \R^+) $,  and $\ f \equiv 1 $. With this settings exists a unique solution $ u \in H^1_0(D, \R) $ and so we can define the solution operator 
	\[  \mathcal{G}: L^{\infty}(D, \R^+) \to H^1_0(D, \R), \qquad \mathcal{G}:a \mapsto u. \]
\end{frame}

%% ======================================

\begin{frame}{Dataset generation}
	We have to generate the dataset $ \left\lbrace a^{(i)}, u^{(i)} \right \rbrace_{i=1}^{N} $.
	\\ \vspace{0.3cm}
	\pause
	\begin{columns}
		\begin{column}{0.8\textwidth}
			For the input we use the push forward of a proper gaussian random fields $ a^{(i)} \sim\mu = T_{\#}N(0, C) $ i.i.d. where $C = -(\Delta + 9I)^{-2}$ and $T:\R \to \R$ such that $T$ gives $12$ to the positive values and $3$ to the negative ones. 
			% \[ T(x) = \begin{cases}
			% 	12\quad & x \ge 0\\
			% 	3 \quad & x < 0 
			% \end{cases}. \]
		\end{column}%
		\begin{column}{0.2\textwidth}
            \includegraphics[width=\textwidth]{FNO_continuation_input.png}
		\end{column}
	\end{columns}
	\vspace{0.3cm}
	\pause
	\begin{columns}
		\begin{column}{0.2\textwidth}
            \includegraphics[width=\textwidth]{FNO_continuation_output.png}
		\end{column}%
		\begin{column}{0.8\textwidth}
			For the output we approximate the solution of the Darcy problem solution $ u^{(i)} = \mathcal{G}(a^{(i)}) $ using the isogeometric geoPDEs library.
		\end{column}
	\end{columns}
\end{frame}

%% ======================================

\begin{frame}{Relative $ L^2 $ error}
	We define the relative $ L^2 $ error as
	\[ \left\| \frac{\mathcal{G} - \mathcal{N}^{*}_{\theta}}{\mathcal{G}}\right\|_{L_{\mu}^2(L^{\infty}, L^{2})} = 	\hspace{0.2cm}\mathbb{E}_{a \sim \mu} \frac{\left\| \mathcal{G}(a) - \mathcal{N}^{*}_{\theta}(a) \right\|^{2}_{L^2(D)}}{\left\| \mathcal{G}(a)\right\|^{2}_{L^2(D)}} \]
	that can be approximated by
	\[\left\| \frac{\mathcal{G} - \mathcal{N}^{*}_{\theta}}{\mathcal{G}}\right\|_{L_{\mu}^2(L^{\infty}, L^{2})} \approx \hspace{0.2cm} \frac1N \sum_{i=1}^{N}\left(  \frac{\sum_{k=1}^{M} \left| u^{(i)}\left( x_k\right)  - \mathcal{N}^{*}_{\theta}\left(a^{(i)}\right)\left( x_k\right)  \right|^{2}}{\sum_{k=1}^{M}\left| u^{(i)}\left( x_k\right)  \right|^{2}}\right). \]
	Where $ \left\lbrace x_{k} \right\rbrace_{k = 1}^{M} \subset D $ is the set of the discretization points. That is a uniform (or Chebyshev) grid of $42$ points per direction in every square patches.
\end{frame}

%% ======================================

% \begin{frame}{Norma relativa $ H^1 $}
% 	\[ \left\| \frac{\mathcal{G} - \mathcal{N}^{*}_{\theta}}{\mathcal{G}}\right\|_{L_{\mu}^2(L^{\infty}, H^{1}_0)} = \mathbb{E}_{a \sim \mu} \frac{\left\| \mathcal{G}(a) - \mathcal{N}^{*}_{\theta}(a) \right\|^{2}_{H^1(D)}}{\left\| \mathcal{G}(a)\right\|^{2}_{H^1(D)}},  \]
% 	dove \[ \left\| f \right\|_{H^1(\mathbb{T}^d)}^2 = \sum_{k \in \Z^d}	\left(1+|k|^2	\right)^s \left| \widehat{f}(k) \right|^2 \]
% 	da cui
% 	\[\left\| \frac{\mathcal{G} - \mathcal{N}^{*}_{\theta}}{\mathcal{G}}\right\|_{L^2_{\mu}(L^{\infty}, H^{1}_0)} \approx\]\[ \approx \frac1N \sum_{i=1}^{N} \frac{\sum_{k \in \Z_{N}}	\left(1+|k|^2 \right)^s \left| \widehat{u^{(i)}}(k) - \widehat{\mathcal{N}_{\theta}^{*}\left(a^{(i)}\right)}(k)\right|^2} {\sum_{k \in \Z_{N}}\left(1+|k|^2	\right)^s \left| \widehat{u^{(i)}}(k) \right|^2}	 \]
% \end{frame}

%% ======================================

\begin{frame}{Fourier continuation numerical experiments}
	\begin{table}[h!] %booktabs
		\centering
		\begin{tabular}{ccc}\toprule
		rel. error $ L^2 $ & parameters & training times\\
			\cmidrule{1-3}
			$ 0.02450 $ & $ 2\, 363\, 681 $ & $ 7 $ hours\\
			\bottomrule
		\end{tabular}
	\end{table}
	\begin{center}
        \begin{minipage}{0.24\textwidth}
            \includegraphics[width=0.93\textwidth]{FNO_continuation_input.png}
        \end{minipage}
        \hfill
        \begin{minipage}{0.24\textwidth}
            \includegraphics[width=\textwidth]{FNO_continuation_output.png}
        \end{minipage}
        \hfill
        \begin{minipage}{0.24\textwidth}
            \includegraphics[width=\textwidth]{FNO_continuation_appro.png}
        \end{minipage}
        \hfill
        \begin{minipage}{0.24\textwidth}
            \includegraphics[width=\textwidth]{FNO_continuation_error.png}
        \end{minipage}
    \end{center}
\end{frame}

%% ======================================

\begin{frame}{Multi-patch neural operator numerical experiments}
	\begin{table}[h!] %booktabs
		\centering
		\begin{tabular}{ccc}\toprule
		rel. error $ L^2 $ & parameters & training times\\
			\cmidrule{1-3}
			$ 0.02450 $ & $ 2\, 363\, 681 $ & $ 7 $ hours\\
			\bottomrule
		\end{tabular}
	\end{table}
	\begin{center}
        \begin{minipage}{0.24\textwidth}
            \includegraphics[width=\textwidth]{MPNO_input.png}
        \end{minipage}
        \hfill
        \begin{minipage}{0.24\textwidth}
            \includegraphics[width=\textwidth]{MPNO_output.png}
        \end{minipage}
        \hfill
        \begin{minipage}{0.24\textwidth}
            \includegraphics[width=\textwidth]{MPNO_appro.png}
        \end{minipage}
        \hfill
        \begin{minipage}{0.24\textwidth}
            \includegraphics[width=\textwidth]{MPNO_error.png}
        \end{minipage}
    \end{center}
\end{frame}

%% ======================================

% \begin{frame}[noframenumbering]{Bibliografia}
% 	\nocite{*}
% 	\printbibliography
% \end{frame}

%% ======================================
\backmatter % last slide

%% ======================================

\appendix
\begin{frame}[noframenumbering]{Universal approximation for $\psi$-FNOs}
	% \blfootnote{\cite{kovachki2021universal}, Kovachki et al., "On universal approximation and error bounds for fourier neural operators".}
	\begin{themedTitleBlock}{Teorema} % \cite{kovachki2021universal}
		Given $ s > d/2 $, $ \lambda \in (0,1) $ and we consider the solution operator for the Darcy problem 
		\[ \mathcal{G}: \mathcal{A}_{\lambda}^{s}(\mathbb{T}^d) \to H^1(\mathbb{T}^d). \]
		Given $ \sigma \in C^3(\R) $ non-polynomial, for any $ N \in \N $ exists $ C > 0 $ and a $ \psi $-FNO
		\[ \mathcal{N}^{*}: \mathcal{A}_{\lambda}^{s}(\mathbb{T}^d) \to H^1(\mathbb{T}^d)  \]
		such that	
		\[ \underset{a \in \mathcal{A}_{\lambda}^{s}}{sup} \left\| \mathcal{G}(a) - \mathcal{N}^{*}(a) \right\|_{H^1(\mathbb{T}^d)} \le CN^{-k} \]
		and $\ \mathrm{depth}(\mathcal{N}^{*}) \le C \log(N) $, $\  \mathrm{lift}(\mathcal{N}^{*}) \le C $, $\ \mathrm{size}(\mathcal{N}^{*}) \lesssim N^d\log(N). $
	\end{themedTitleBlock}
\end{frame}

%% ======================================

\begin{frame}[noframenumbering]{Trasformata di Fourier}
	\centering
	\begin{itemize}
		\item  Given $ v \in  L^2(\mathbb{T}^d) $, the Fourier transform is defined as 
		\[ \begin{split}
			\mathcal{F} : \ & L^{2}(\mathbb{T}^d, \C^{n})\to \ell^{2}(\Z^d, \C^{n})\\
			& v \mapsto \mathcal{F}(v)
		\end{split} \]
		\[ \mathcal{F}(v)(k) := \frac{1}{(2 \pi)^{d}} \int_{\mathbb{T}^{d}}v(x) e^{-i \left\langle k , x \right\rangle} \ dx,  \quad \forall k \in \Z^d. \]
		\item Given $ \widehat{v} = \left\lbrace \widehat{v}_k\right\rbrace_{k \in \Z^d} \in \ell^{2}(\Z^d, \C^{n}) $, the anti-Fourier transform is defined as 
		\[ \begin{split}
			\mathcal{F}^{-1} : \ & \ell^{2}(\Z^d, \C^{n}) \to L^{2}(D, \C^{n})\\
			& \widehat{v} \mapsto \mathcal{F}^{-1}(\widehat{v})
		\end{split} \]
		\[ (\mathcal{F}^{-1}\widehat{v})(x) = \sum_{k \in \Z^d} \widehat{v}_k e^{i \left\langle k , x \right\rangle } \qquad \forall x \in D. \]
	\end{itemize}
\end{frame}

%% ======================================

\begin{frame}[noframenumbering]{Discrete Fourier transform and anti-transform}
	We choose $ N \in \N $ and fix an uniform grid $ \{x_j\}_{j \in \mathcal{I}_N} $ with $ x_j = (2\pi j)/(2N+1) \in \mathbb{T}^d $, $ j \in \mathcal{I}_N = \{0, \dots, 2N\}^{d} $, finally we define $\mathcal{K}_N := \{k \in \Z^d : |k|_{\infty} \le N\}$.
	We define the discrete Fourier transform as
	\[ \mathcal{F}_{N}: \R^{\mathcal{I}_N} \to \C^{\mathcal{K}_N}\]\[ \mathcal{F}_{N}(v)(k) := \frac{1}{(2N+1)^{d}} \sum_{j \in \mathcal{I}_N}v(x_j) e^{-2\pi i \left\langle j, k \right\rangle/N }, \quad \forall k \in \mathcal{K}_N, \]
	and we define the discret version of the anti-Fourier transform as
	\[ \mathcal{F}_{N}^{-1}: \C^{\mathcal{K}_N} \to \R^{\mathcal{I}_N}\]\[ \mathcal{F}_{N}^{-1}(\widehat{v})(j) := \sum_{k \in \mathcal{K}_N} \widehat{v}_k e^{2\pi i \left\langle j, k \right\rangle/N }, \qquad \forall j \in \mathcal{J}_N. \]
\end{frame}

%% ======================================

\begin{frame}[noframenumbering]{Sketch of the proof of universal approximation theorem for FNOs}
	\centering
	\begin{itemize}
		\item The definition of the projection on the trigonometric polynomial is
		\[ P_N: L^{2}(\mathbb{T}^d) \to L^{2}_N(\mathbb{T}^d),  \]
		\[ P_N\left(\sum_{k \in \Z^d} c_k e^{i \left\langle x, k \right\rangle } \right) = \sum_{|k|_{\infty}\le N} c_k e^{i \left\langle x, k \right\rangle }, \qquad \forall (c_k)_{k \in \Z^d} \in \ell^2(\Z^d). \]
		\item If the universal approximation theorem holds for $ s'=0 $ then it holds for any value of $ s' \ge 0 $.
		\item We fix $ s' = 0 $  \[ \mathcal{G}_{N}: H^{s}(\mathbb{T}^d, \R^{d_a}) \to L^2(\mathbb{T}^d, \R^{d_u}), \qquad \mathcal{G}_{N}(a):= P_N  \mathcal{G} (P_Na), \]
		holds that $ \forall \varepsilon > 0 $, $ \exists N \in \N $ such that
		\[ \left\|\mathcal{G}(a) - \mathcal{G}_N(a) \right\|_{L^2} \le \varepsilon, \qquad \forall a \in K. \]
	\end{itemize}
\end{frame}

%% ======================================

\begin{frame}[noframenumbering]{Schema dimostrazione teo. universale FNO}
	Definiamo l'operatore
	\[ \widehat{\mathcal{G}}_N: \C^{\mathcal{K}_N} \to\C^{\mathcal{K}_N}, \qquad \widehat{\mathcal{G}}_N(\widehat{a}_k) := \mathcal{F}_N (\mathcal{G}_N ( Re (\mathcal{F}_N^{-1}(\widehat{a}_k)))), \]
	per il quale vale l'identitÃ 
	\[ \mathcal{G}_N (a) = \mathcal{F}_N^{-1} \circ \widehat{\mathcal{G}}_N \circ \mathcal{F}_N (P_Na), \]
	per le funzioni $  a \in L^2(\mathbb{T}^d, \R^{d_a}) $. Ci si riconduce a dimostrare che gli operatori neurali di Fourier possono approssimare
	\[ \mathcal{F}_N^{-1}, \ \widehat{\mathcal{G}}_N , \ \mathcal{F}_N (P_Na). \]
\end{frame}

%% ======================================

\begin{frame}[noframenumbering]{Definizione MLP}
	Let $ d \in \N $ and $ L\in \N $ with $ L \ge 2 $ and $ \sigma : \R \to \R  $ a non-linear activation function. Let $ A_{\ell} \in \R^{n_{\ell}\times n_{\ell-1}} $, $ b_{\ell} \in \R^{\ell} $ with $ n_{\ell}\in \N $ for $ \ell = 1, \dots, L $	and $ n_{0} = d $. We call multilayer perceptron (MLP) the function defined as
	\[ \begin{cases}
		x_{L} = A_{L}x_{L-1} + b_{L} \qquad & \\
		x_{\ell} = \sigma\left( A_{\ell}x_{\ell-1} + b_{\ell} \right) 
	\end{cases}, \]
	where $ x_0 $ is the input and $ x_{L} $ is the output of the function.
\end{frame}

%% ======================================

\begin{frame}[noframenumbering]{Universal approximation theorem for operator}
	Suppose that $ \sigma \in TW $, $ X $ is a Banach space, $ K_1 \subset X $, $ K_2 \subset \R^d $ are two compact sets in $ X $ and $ \R^{d} $ respectively, and $ V $ is a compact set in $ C(K_1) $. Let $ G $ a nonlinear continuous operator which maps $ V $ into $ C(K_2) $, then for any $ \varepsilon > 0 $, there are a positive integers $ n $, $ p $, $ m $; real constants $ c_i^{k}$, $ \theta_i^{k}$, $ \xi_{ij}^{k} $, $ \zeta^k \in \R $, points $ w^{k} \in \R^d $ and $ x_j \in K_1 $, with $ i = 1, \dots, n$, $ j = 1, \dots, m $ and $ k = 1, \dots, p $, such that
	\[ \left| G(u)(y) - \sum_{k=1}^{p}\sum_{i=1}^{n}c_i^k \sigma \left( \sum_{j=1}^{m} \xi_{ij}^k u(x_j) + \theta_i^k \right) \sigma(w^k \cdot y + \zeta^k) \right| < \varepsilon  \]
	holds for all $ u \in V $ and $ y \in K_2 $.
\end{frame}

\end{document}


To begin a  new line with some fixed vertical space
\vspace{\baselineskip}

\begin{block}{title}
\end{block}


% \begin{verbatim}  This begins a verbatim environment where the text is displayed exactly as it is written. This is useful for showing code snippets


% \begin{itemize}[<+->]  This starts an itemized (bulleted) list. The [<+->] option specifies that each item in the list should be revealed one by one as the presentation progresses (incrementally).


% \begin{frame}[fragile]{Writing a Simple Slide}  This starts a new slide (frame) titled "Writing a Simple Slide". The fragile option is used because the frame will contain verbatim text, which needs special handling in Beamer.


\uncover<2->{ <content> }  this means that the `content` is veasible from the second transition of the slide
The \uncover command makes the text visible starting from the specified slide number.


\begin{columns} % adding [onlytextwidth] the left margins will be set correctly
    \begin{column}{0.50\textwidth}
        
    \end{column}%
    \begin{column}{0.50\textwidth}
        
    \end{column}
\end{columns}


\textcolor{<color name>}{text}, \emph{}, \alert{} --> commands to brings the attention somewhere


% for a single monocrome block
\begin{themedColorBlock}{ `title` }
    `content`
\end{themedColorBlock}


% for block with highlighted title
\begin{themedTitleBlock}{ `title` }
    `content`
\end{themedTitleBlock}


% implemented styles for block's color
\themecolor{Black&White}, \themecolor{Red}, \themecolor{Yellow}, \themecolor{GreeenBrown}, 
\themecolor{DarkBlue}, \themecolor{Orange}, \themecolor{BlueGreen}, \themecolor{Green}, \themecolor{Blue}